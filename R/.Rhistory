##     Statistically speaking, the cross validation error for
##     lambda.1se is indistinguisable from the cross validation error
##     for lambda.min, since they are within 1 SE of each other.
##     So we can pick the simpler model without
##     much risk of severely hindering the ability to accurately
##     predict values for 'y' given values for 'x'.
##
##     All that said, lambda.1se only makes the model simpler when
##     alpha != 0, since we need some Lasso regression mixed in
##     to remove variables from the model. However, to keep things
##     consistant when we compare different alphas, it makes sense
##     to use lambda.1se all the time.
##
## newx = is the Testing Dataset
## Lastly, let's calculate the Mean Squared Error (MSE) for the model
## created for alpha = 0.
## The MSE is the mean of the sum of the squared difference between
## the predicted 'y' values and the true 'y' values in the
## Testing dataset...
mean((y.test - alpha0.predicted)^2)
################################
##
## alpha = 1, Lasso Regression
##
################################
alpha1.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
alpha=1, family="gaussian")
alpha1.predicted <-
predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx=x.test)
mean((y.test - alpha1.predicted)^2)
################################
##
## alpha = 0.5, a 50/50 mixture of Ridge and Lasso Regression
##
################################
alpha0.5.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
alpha=0.5, family="gaussian")
alpha0.5.predicted <-
predict(alpha0.5.fit, s=alpha0.5.fit$lambda.1se, newx=x.test)
mean((y.test - alpha0.5.predicted)^2)
################################
##
## However, the best thing to do is just try a bunch of different
## values for alpha rather than guess which one will be best.
##
## The following loop uses 10-fold Cross Validation to determine the
## optimal value for lambda for alpha = 0, 0.1, ... , 0.9, 1.0
## using the Training dataset.
##
## NOTE, on my dinky laptop, this takes about 2 minutes to run
##
################################
list.of.fits <- list()
for (i in 0:10) {
## Here's what's going on in this loop...
## We are testing alpha = i/10. This means we are testing
## alpha = 0/10 = 0 on the first iteration, alpha = 1/10 = 0.1 on
## the second iteration etc.
## First, make a variable name that we can use later to refer
## to the model optimized for a specific alpha.
## For example, when alpha = 0, we will be able to refer to
## that model with the variable name "alpha0".
fit.name <- paste0("alpha", i/10)
## Now fit a model (i.e. optimize lambda) and store it in a list that
## uses the variable name we just created as the reference.
list.of.fits[[fit.name]] <-
cv.glmnet(x.train, y.train, type.measure="mse", alpha=i/10,
family="gaussian")
}
## Now we see which alpha (0, 0.1, ... , 0.9, 1) does the best job
## predicting the values in the Testing dataset.
results <- data.frame()
for (i in 0:10) {
fit.name <- paste0("alpha", i/10)
## Use each model to predict 'y' given the Testing dataset
predicted <-
predict(list.of.fits[[fit.name]],
s=list.of.fits[[fit.name]]$lambda.1se, newx=x.test)
## Calculate the Mean Squared Error...
mse <- mean((y.test - predicted)^2)
## Store the results
temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
results <- rbind(results, temp)
}
## View the results
results
##############################################################
##
## Example 2
## 3500 useless variables, 1500 useful (so lots of useful variables)
## 1,000 samples and 5,000 parameters
##
##############################################################
set.seed(42) # Set seed for reproducibility
n <- 1000    # Number of observations
p <- 5000     # Number of predictors included in model
real_p <- 1500  # Number of true predictors
## Generate the data
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
# Split data into train (2/3) and test (1/3) sets
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]
y.train <- y[train_rows]
y.test <- y[-train_rows]
list.of.fits <- list()
for (i in 0:10) {
fit.name <- paste0("alpha", i/10)
list.of.fits[[fit.name]] <-
cv.glmnet(x.train, y.train, type.measure="mse", alpha=i/10,
family="gaussian")
}
results <- data.frame()
for (i in 0:10) {
fit.name <- paste0("alpha", i/10)
predicted <-
predict(list.of.fits[[fit.name]],
s=list.of.fits[[fit.name]]$lambda.1se, newx=x.test)
mse <- mean((y.test - predicted)^2)
temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
results <- rbind(results, temp)
}
results
View(x)
View(list.of.fits)
source("~/GitHub/HoustonCrimeDataAnalysis/R/NIBRS_HPD_ETL.R")
source("~/GitHub/HoustonCrimeDataAnalysis/R/NIBRS_HPD_ETL.R")
library(gt)
setwd('C:/Users/chris/Documents/Houston_Crime_Data_Analysis/August2023')
bas_yr='2019'
pri_yr='2022'
cur_yr='2023'
latest_mon='08'
base_end_dt<-eom(latest_mon,bas_yr)
pri_end_dt<-eom(latest_mon,pri_yr)
cur_end_dt<-eom(latest_mon,cur_yr)
incidents_ytd<-multi_year[ (NIBRSDescription %chin% violent_crimes),]%>%
.[( RMSOccurrenceDate>=glue('{bas_yr}-01-01')
&RMSOccurrenceDate<=base_end_dt)
|( RMSOccurrenceDate>=glue('{pri_yr}-01-01')
&RMSOccurrenceDate<=pri_end_dt)
|( RMSOccurrenceDate>=glue('{cur_yr}-01-01')
&RMSOccurrenceDate<=cur_end_dt),]
#Aggregate
incidents_ytd_agg<-incidents_ytd[,.(OffenseCount=sum(OffenseCount))
,by=list(Beat,year)]
#Transpose Column per year
comp_ytds<-dcast( incidents_ytd_agg
,Beat~year
,value.var = c("OffenseCount"))
#Merge GeoJson to DataTable
##This filters to mappable beats from data.
beats_tab<-setDT(beats)
final<-comp_ytds[beats_tab,on=.(Beat=Beats)]
#Handle Numeric NAs
final[is.na(`2019`), `2019`:= 0]
final[is.na(`2022`), `2022`:= 0]
final[is.na(`2023`), `2023`:= 0]
#Differnces
final$diff_prior<-final$'2023'-final$'2022'
final$diff_base<-final$'2023'-final$'2019'
final$crime_density<-final$`2023`/final$Area_sq_mi
#POPUP
final$popup<-paste("<b>HPD Beat: </b>", final$Beat, "<br>",
"<br>", "<b>Change from Last Year: </b>", final$diff_prior,
"<br>", "<b>Change from 2019: </b>", final$diff_base,
"<br>", "<b>Offense Count 2019: </b>", final$'2019',
"<br>", "<b>Offense Count 2022: </b>", final$'2022',
"<br>", "<b>Offense Count 2023: </b>", final$'2023')
#SIMPLE REPORT
rep_data<-final[,c('Beat','diff_base','diff_prior')]
rep_data[diff_base==0,base_chg:='Zero']
rep_data[diff_base>0,base_chg:='Up']
rep_data[diff_base<0,base_chg:='Down']
rep_data[diff_prior==0,prior_chg:='Zero']
rep_data[diff_prior>0,prior_chg:='Up']
rep_data[diff_prior<0,prior_chg:='Down']
rep_data_base_agg<-rep_data[,.(Freq=.N,Count=sum(diff_base)),by='base_chg' ]
rep_data_prior_agg<-rep_data[,.(Freq=.N,Count=sum(diff_prior)),by='prior_chg' ]
rep_data_prior_agg%>%gt()%>%tab_header(
title='Beat Changes from Prior Year',
subtitle='2022')%>%
cols_label( prior_chg = md("**Change Direction**")
,Freq=md("**Count**")
,Count=md("**Offense Count**"))
ggplot( data=rep_data_prior_agg
,aes( y=Freq
,x=prior_chg))+
geom_bar( position="dodge"
,stat="identity")+
ggtitle(glue("Beat Changes from Prior Year(2022)"))+
labs(x="Direction of Change",y="Number of Beats")+
theme_economist()
rep_data_base_agg%>%gt()%>%tab_header(
title='Beat Changes from Base Year',
subtitle='2019')%>%
cols_label( base_chg=md("**ChangeDirection**")
,Freq=md("**Count**")
,Count=md("**Offense Count**"))
ggplot( data=rep_data_base_agg
,aes( y=Freq
,x=base_chg))+
geom_bar( position="dodge"
,stat="identity")+
ggtitle(glue("Beat Changes from Base Year(2019)"))+
theme_economist()
rep_data_prior_agg$Beat_Prec<-round((rep_data_prior_agg$Freq/sum(rep_data_prior_agg$Freq))*100.00,1)
print(sum(rep_data_prior_agg$Count))
# MAPPING
#Prep Final
final<- sf::st_as_sf(final)
#Get Bounds for custom bins
min_bin<-min(final$diff_prior,na.rm=TRUE)
max_bin<-max(final$diff_prior,na.rm=TRUE)
c_bins=c( min_bin
,(min_bin%/%3)*2
,min_bin%/%3
,0
,max_bin%/%3
,(max_bin%/%3)*2
,max_bin)
# Creating a color palette, with custom bins, based on difference from prior year
pal <- colorBin( "RdYlBu"
,domain=final$diff_prior
,c_bins
,pretty = FALSE
,reverse=TRUE)
change<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$diff_prior),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$diff_prior,
position = "bottomright",
title = "YTD Changes in Violent Crime Counts")
saveWidget( change
,selfcontained=TRUE
,file=glue('Violent_Change_{latest_mon}.html')
,title=glue('Violent_Change_{latest_mon}'))
## BASE
min_bin<-min(final$diff_base,na.rm=TRUE)
max_bin<-max(final$diff_base,na.rm=TRUE)
c_bins=c( min_bin
,(min_bin%/%3)*2
,min_bin%/%3
,0
,max_bin%/%3
,(max_bin%/%3)*2
,max_bin)
pal <- colorBin( "RdYlBu"
,domain=final$diff_base
,c_bins
,pretty = FALSE
,reverse=TRUE)
change_base<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$diff_base),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$diff_base,
position = "bottomright",
title = "YTD Changes in Violent Crime Counts 2019")
saveWidget( change_base
,selfcontained=TRUE
,file=glue('Violent_Change_Base_{latest_mon}.html')
,title=glue('Violent_Change_Base_{latest_mon}'))
## DENSITY
pal <- colorQuantile( palette="RdYlBu"
,domain=final$crime_density
,n=5
,reverse=TRUE)
dense<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$crime_density),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$crime_density,
position = "bottomright",
title = "Violent Crime Density(#/Square Mile)")
saveWidget( dense
,selfcontained=TRUE
,file=glue('Violent_Density_{latest_mon}.html')
,title=glue('Violent_Density_{latest_mon}'))
View(final)
View(final)
View(final)
## DENSITY
pal <- colorQuantile( palette="RdYlBu"
,domain=final$crime_density
,n=10
,reverse=TRUE)
dense<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$crime_density),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$crime_density,
position = "bottomright",
title = "Violent Crime Density(#/Square Mile)")
## DENSITY
pal <- colorQuantile( palette="RdYlBu"
,domain=final$crime_density
,n=6
,reverse=TRUE)
dense<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$crime_density),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$crime_density,
position = "bottomright",
title = "Violent Crime Density(#/Square Mile)")
saveWidget( dense
,selfcontained=TRUE
,file=glue('Violent_Density_{latest_mon}.html')
,title=glue('Violent_Density_{latest_mon}'))
## DENSITY
pal <- colorQuantile( palette="RdYlBu"
,domain=final$crime_density
,n=7
,reverse=TRUE)
dense<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$crime_density),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$crime_density,
position = "bottomright",
title = "Violent Crime Density(#/Square Mile)")
saveWidget( dense
,selfcontained=TRUE
,file=glue('Violent_Density_{latest_mon}.html')
,title=glue('Violent_Density_{latest_mon}'))
## DENSITY
pal <- colorQuantile( palette="RdYlBu"
,domain=final$crime_density
,n=8
,reverse=TRUE)
dense<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$crime_density),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$crime_density,
position = "bottomright",
title = "Violent Crime Density(#/Square Mile)")
## DENSITY
pal <- colorQuantile( palette="RdYlBu"
,domain=final$crime_density
,n=6
,reverse=TRUE)
dense<-leaflet() %>%
addTiles()%>%
addTiles(group = "OSM (default)") %>%
addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
addLayersControl(
baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
options = layersControlOptions(collapsed = FALSE))%>%
addPolygons(data = final,
fillColor = ~pal(final$crime_density),
fillOpacity = 0.7,
weight = 1.0,
smoothFactor = 0.2,
popup = ~popup) %>%
addLegend(pal = pal,
values = final$crime_density,
position = "bottomright",
title = "Violent Crime Density(#/Square Mile)")
saveWidget( dense
,selfcontained=TRUE
,file=glue('Violent_Density_{latest_mon}.html')
,title=glue('Violent_Density_{latest_mon}'))
View(final)
View(final)
source("~/GitHub/HoustonCrimeDataAnalysis/R/NIBRS_HPD_ETL.R")
year4[,.(freq=sum(OffenseCount)),by=c('Beat')]
setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
View(qa)
min(year4$RMSOccurrenceDate)
max(year4$RMSOccurrenceDate)
setDT(year4)[,.(freq=sum(OffenseCount)),by=c('NIBRSDescription')]
setDT(year4)[,.(freq=sum(OffenseCount)),by=c('NIBRSDescription','Beat')]
qa_desc<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('NIBRSDescription','Beat')]
View(qa_desc)
View(qa_desc)
View(qa_desc)
View(qa_desc)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
View(qa_beat_23)
View(qa_beat_23)
View(qa_beat_22)
View(qa_beat_22)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<-qa_beat_23[qa_beat_22, on = .(Beat)]
View(compare)
View(compare)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<fullOuterJoin <- merge(qa_beat_23, qa_beat_22, all=TRUE)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<-merge(qa_beat_23, qa_beat_22, all=TRUE)
View(compare)
View(compare)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<-merge(qa_beat_23, qa_beat_22, all=TRUE,by=c('Beat'))
View(compare)
View(compare)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<-merge(qa_beat_23, qa_beat_22, all=TRUE,by=c('Beat'))
exp_miss<-compare[is.na(freq.x)]
new_not<-compare[is.na(freq.y)]
View(exp_miss)
View(exp_miss)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<-merge(qa_beat_23, qa_beat_22, all=TRUE,by=c('Beat'))
exp_miss<-compare[is.na(freq.x)]
new_not<-compare[is.na(freq.y)]
print(exp_miss)
print(not_new)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<-merge(qa_beat_23, qa_beat_22, all=TRUE,by=c('Beat'))
exp_miss<-compare[is.na(freq.x)]
new_not<-compare[is.na(freq.y)]
print(exp_miss)
print(new_not)
qa_beat_23<-setDT(year4)[,.(freq=sum(OffenseCount)),by=c('Beat')]
qa_beat_22<-setDT(year3)[,.(freq=sum(OffenseCount)),by=c('Beat')]
compare<-merge(qa_beat_23, qa_beat_22, all=TRUE,by=c('Beat'))
exp_miss<-compare[is.na(freq.x),.(Beat)]
new_not<-compare[is.na(freq.y),.(Beat)]
print(exp_miss)
print(new_not)
