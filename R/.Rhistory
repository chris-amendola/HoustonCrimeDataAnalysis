# Read list of pdf_files to text scanned
BPD_DATA_RAW<-read_csv( glue('{data_src}raw_data_files_txt_list.txt')
,col_names=FALSE)
BPD_DATA_RAW$file<-glue('{basename(dirname(BPD_DATA_RAW$X1))}/{basename(BPD_DATA_RAW$X1)}')
i<-0
for (file in BPD_DATA_RAW$file){
i<-i+1
print(glue('{data_src}{file}'))
data<-pdf_parse(glue('{data_src}{file}'))
data<-sapply(data,line_clean)
lapply(data, write,glue('{out_lib}{basename(file)}'), append=TRUE)
print(i)
}
# Read list of pdf_files to OCR'ed
BPD_DATA_RAW<-read_csv( glue('{data_src}raw_data_files_ocr_list.txt')
,col_names=FALSE)
BPD_DATA_RAW$file<-glue('{basename(dirname(BPD_DATA_RAW$X1))}/{basename(BPD_DATA_RAW$X1)}')
i<-0
for (file in BPD_DATA_RAW$file){
i<-i+1
print(glue('{data_src}{file}'))
data<-pdf_parse(glue('{data_src}{file}'),ocr=TRUE)
data<-sapply(data,line_clean)
lapply(data, write,glue('{out_lib}{basename(file)}'), append=TRUE)
print(i)
}
View(data)
line_clean<-function(line,delm='\\|'){
num_cols<-8
# locating the places of comma occurrence
loc_delm<-str_locate_all(line, delm)[[1]]
# calculating the number of positions
# in comma array
delm_ele<-nrow(loc_delm)
# fetching the last delm position
last_loc_delm<-loc_delm[delm_ele,]
if (delm_ele!=num_cols){
if ((num_cols-delm_ele)==1){
rpl_str<-'||'
} else
if ((num_cols-delm_ele)==2){
rpl_str<-'|||'
}
else{
print('!!BAD!!')
rpl_str<-'!<LOOK>!|'
}
# replace the last comma with & symbol
str_sub( line
,last_loc_delm[1]
,last_loc_delm[2])<-rpl_str
}
return(line)}
test_line<-'230901-0030|09/01/23|08:09|7800 W IH 610 S|DAV|09:13|Info'
line_clean(test_line)
source("~/Bellaire_Crime_Data/line_clean.R")
source("C:/Users/chris/Documents/Bellaire_Crime_Data/parse_dfs_pdfs.R")
library(pdftools)
library(tidyverse)
library(readr)
library(glue)
library(stringr)
library(data.table)
data_src<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/'
out_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/parsed/'
col_list<-c('CFS','Date','Time','Location','Call Type','Enroute','Arrived','Closed','Disposition')
#Gather Raw Data/PDFS :(
# Read list of pdf_files to text scanned
BPD_DATA_RAW<-read_csv( glue('{data_src}raw_data_files_txt_list.txt')
,col_names=FALSE)
BPD_DATA_RAW$file<-glue('{basename(dirname(BPD_DATA_RAW$X1))}/{basename(BPD_DATA_RAW$X1)}')
i<-0
for (file in BPD_DATA_RAW$file){
i<-i+1
print(glue('{data_src}{file}'))
data<-pdf_parse(glue('{data_src}{file}'))
data<-sapply(data,line_clean)
#lapply(data, write,glue('{out_lib}{basename(file)}'), append=TRUE)
print(data)
print(i)
}
source("C:/Users/chris/Documents/Bellaire_Crime_Data/parse_dfs_pdfs.R")
source("~/Bellaire_Crime_Data/line_clean.R")
library(pdftools)
library(tidyverse)
library(readr)
library(glue)
library(stringr)
library(data.table)
data_src<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/'
out_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/parsed/'
col_list<-c('CFS','Date','Time','Location','Call Type','Enroute','Arrived','Closed','Disposition')
#Gather Raw Data/PDFS :(
# Read list of pdf_files to text scanned
BPD_DATA_RAW<-read_csv( glue('{data_src}raw_data_files_txt_list.txt')
,col_names=FALSE)
BPD_DATA_RAW$file<-glue('{basename(dirname(BPD_DATA_RAW$X1))}/{basename(BPD_DATA_RAW$X1)}')
i<-0
for (file in BPD_DATA_RAW$file){
i<-i+1
print(glue('{data_src}{file}'))
data<-pdf_parse(glue('{data_src}{file}'))
data<-sapply(data,line_clean)
#lapply(data, write,glue('{out_lib}{basename(file)}'), append=TRUE)
print(i)
}
source("C:/Users/chris/Documents/Bellaire_Crime_Data/parse_dfs_pdfs.R")
library(pdftools)
library(tidyverse)
library(readr)
library(glue)
library(stringr)
library(data.table)
data_src<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/'
out_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/parsed/'
col_list<-c('CFS','Date','Time','Location','Call Type','Enroute','Arrived','Closed','Disposition')
#Gather Raw Data/PDFS :(
# Read list of pdf_files to text scanned
BPD_DATA_RAW<-read_csv( glue('{data_src}raw_data_files_txt_list.txt')
,col_names=FALSE)
BPD_DATA_RAW$file<-glue('{basename(dirname(BPD_DATA_RAW$X1))}/{basename(BPD_DATA_RAW$X1)}')
i<-0
for (file in BPD_DATA_RAW$file){
i<-i+1
print(glue('{data_src}{file}'))
data<-pdf_parse(glue('{data_src}{file}'))
data<-sapply(data,line_clean)
#lapply(data, write,glue('{out_lib}{basename(file)}'), append=TRUE)
print(i)
}
# Read list of pdf_files to OCR'ed
BPD_DATA_RAW<-read_csv( glue('{data_src}raw_data_files_ocr_list.txt')
,col_names=FALSE)
BPD_DATA_RAW$file<-glue('{basename(dirname(BPD_DATA_RAW$X1))}/{basename(BPD_DATA_RAW$X1)}')
i<-0
for (file in BPD_DATA_RAW$file){
i<-i+1
print(glue('{data_src}{file}'))
data<-pdf_parse(glue('{data_src}{file}'),ocr=TRUE)
data<-sapply(data,line_clean)
#lapply(data, write,glue('{out_lib}{basename(file)}'), append=TRUE)
print(i)
}
library(pdftools)
library(tidyverse)
library(readr)
library(glue)
library(stringr)
library(data.table)
data_src<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/'
out_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/DATA/parsed/'
col_list<-c('CFS','Date','Time','Location','Call Type','Enroute','Arrived','Closed','Disposition')
#Gather Raw Data/PDFS :(
# Read list of pdf_files to text scanned
BPD_DATA_RAW<-read_csv( glue('{data_src}raw_data_files_txt_list.txt')
,col_names=FALSE)
BPD_DATA_RAW$file<-glue('{basename(dirname(BPD_DATA_RAW$X1))}/{basename(BPD_DATA_RAW$X1)}')
i<-0
for (file in BPD_DATA_RAW$file){
i<-i+1
print(glue('{data_src}{file}'))
data<-pdf_parse(glue('{data_src}{file}'))
data<-sapply(data,line_clean)
lapply(data, write,glue('{out_lib}{basename(file)}'), append=TRUE)
print(i)
}
# Print the filtered matrix
print(filtered_matrix)
filter_matrix_by_column <- function(matrix, column_name, dataframe, filter_column_name) {
# Get the filter values from the specified column in the data frame
filter_values <- dataframe[[filter_column_name]]
# Filter the matrix based on the specified column name and filter values
filtered_matrix <- matrix[matrix[, column_name] %in% filter_values, ]
# Return the filtered matrix
return(filtered_matrix)
}
matrix <- matrix(
c("A", "B", "C", "D", "E", "F"),
nrow = 3,
ncol = 2,
dimnames = list(NULL, c("Origin.Zip", "OtherColumn"))
)
# Create a sample data frame
dataframe <- data.frame(
PC = c("B", "D")
)
# Filter the matrix based on the "Origin.Zip" column using values from the "PC" column in the data frame
filtered_matrix <- filter_matrix_by_column(matrix, "Origin.Zip", dataframe, "PC")
# Print the filtered matrix
print(filtered_matrix)
filter_matrix_by_column <- function(matrix, column_name, dataframe, filter_column_name) {
# Get the filter values from the specified column in the data frame
filter_values <- dataframe[[filter_column_name]]
# Filter the matrix based on the specified column name and filter values
filtered_matrix <- matrix[!is.na(matrix[, column_name]), ]
# Return the filtered matrix
return(filtered_matrix)
}
# EXAMPLE USAGE
# Create a sample matrix
matrix <- matrix(
c("A", "B", "C", "D", "E", "F"),
nrow = 3,
ncol = 2,
dimnames = list(NULL, c("Origin.Zip", "OtherColumn"))
)
# Create a sample data frame
dataframe <- data.frame(
PC = c("B", "D")
)
# Filter the matrix based on the "Origin.Zip" column using values from the "PC" column in the data frame
filtered_matrix <- filter_matrix_by_column(matrix, "Origin.Zip", dataframe, "PC")
# Print the filtered matrix
print(filtered_matrix)
source("~/GitHub/HoustonCrimeDataAnalysis/R/NIBRS_HPD_ETL.R")
library(gt)
range<-500
#4955 BEECHNUT STREET
#4807 Pin Oak Park, Houston, TX 77081
location<-multi_year[(  StreetName=='PIN OAK PARK'
#& StreetType=='BLVD'
& StreetNo==4807
& year==2023)]
min(location$MapLatitude ,na.rm=TRUE)
max(location$MapLatitude ,na.rm=TRUE)
min(location$MapLongitude ,na.rm=TRUE)
max(location$MapLongitude ,na.rm=TRUE)
#-----------#
#-- Look for nearby --#
# Get long and lat for the address
loci_lat<-mean(location$MapLatitude ,na.rm=TRUE)
loci_lon<-mean(location$MapLongitude ,na.rm=TRUE)
# Set Manual
#-loci_lat<-29.685243003232415
#-loci_lon<- -95.47805340848808
# Get degrees for distance around the location coords
lat_range<-range/111111
lon_range<-(range/111111)#*(cos((loci_lat*3.1415926)/180))
#If your displacements aren't too great (less than a few kilometers) and you're not right at the
#poles, use the quick and dirty estimate that 111,111 meters (111.111 km) in the y direction is 1
#degree (of latitude) and 111,111 * cos(latitude) meters in the x direction is 1 degree (of
#longitude).
#  10 degrees = 10 * pi / 180 radians
box_lon_min<-loci_lon-lon_range
box_lon_max<-loci_lon+lon_range
box_lat_min<-loci_lat-lat_range
box_lat_max<-loci_lat+lat_range
# GET ALL in area
area<-multi_year[(  MapLongitude>=box_lon_min
& MapLongitude<=box_lon_max
& MapLatitude>=box_lat_min
& MapLatitude<=box_lat_max
& year==2023)]
area_sum<-area[,.(OffenseCount=sum(OffenseCount),IncidentCount=.N)
,by=c('NIBRSDescription')]
# Check Locations on a map
plot_map<-area[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))]%>%
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
datax<-st_join( plot_map
,districts
,join = st_within)
geo_plot(datax)
# Tabular Report
area_sum%>%gt()%>%tab_header(
title=glue('HPD Incident Reports 2023 (January-October)'),
subtitle=glue('{sum(area_sum$IncidentCount)} Incidents - {sum(area_sum$OffenseCount)} Offenses'))#%>%
#cols_label( base_chg=md("**ChangeDirection**")
#            ,Freq=md("**Count**")
#            ,Count=md("**Offense Count**"))
#SAVE A CSV
View(location)
View(location)
library(gt)
range<-500
#4955 BEECHNUT STREET
#4807 Pin Oak Park, Houston, TX 77081
location<-multi_year[(  StreetName=='PIN OAK PARK'
#& StreetType=='BLVD'
#& StreetNo==4807
& year==2023)]
min(location$MapLatitude ,na.rm=TRUE)
max(location$MapLatitude ,na.rm=TRUE)
min(location$MapLongitude ,na.rm=TRUE)
max(location$MapLongitude ,na.rm=TRUE)
#-----------#
#-- Look for nearby --#
# Get long and lat for the address
loci_lat<-mean(location$MapLatitude ,na.rm=TRUE)
loci_lon<-mean(location$MapLongitude ,na.rm=TRUE)
# Set Manual
#-loci_lat<-29.685243003232415
#-loci_lon<- -95.47805340848808
# Get degrees for distance around the location coords
lat_range<-range/111111
lon_range<-(range/111111)#*(cos((loci_lat*3.1415926)/180))
#If your displacements aren't too great (less than a few kilometers) and you're not right at the
#poles, use the quick and dirty estimate that 111,111 meters (111.111 km) in the y direction is 1
#degree (of latitude) and 111,111 * cos(latitude) meters in the x direction is 1 degree (of
#longitude).
#  10 degrees = 10 * pi / 180 radians
box_lon_min<-loci_lon-lon_range
box_lon_max<-loci_lon+lon_range
box_lat_min<-loci_lat-lat_range
box_lat_max<-loci_lat+lat_range
# GET ALL in area
area<-multi_year[(  MapLongitude>=box_lon_min
& MapLongitude<=box_lon_max
& MapLatitude>=box_lat_min
& MapLatitude<=box_lat_max
& year==2023)]
area_sum<-area[,.(OffenseCount=sum(OffenseCount),IncidentCount=.N)
,by=c('NIBRSDescription')]
# Check Locations on a map
plot_map<-area[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))]%>%
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
datax<-st_join( plot_map
,districts
,join = st_within)
geo_plot(datax)
# Tabular Report
area_sum%>%gt()%>%tab_header(
title=glue('HPD Incident Reports 2023 (January-October)'),
subtitle=glue('{sum(area_sum$IncidentCount)} Incidents - {sum(area_sum$OffenseCount)} Offenses'))#%>%
#cols_label( base_chg=md("**ChangeDirection**")
#            ,Freq=md("**Count**")
#            ,Count=md("**Offense Count**"))
#SAVE A CSV
View(location)
View(location)
library(gt)
range<-500
#4955 BEECHNUT STREET
#4807 Pin Oak Park, Houston, TX 77081
location<-multi_year[(  StreetName=='PIN OAK PARK'
#& StreetType=='BLVD'
& StreetNo==4807
& year==2023)]
min(location$MapLatitude ,na.rm=TRUE)
max(location$MapLatitude ,na.rm=TRUE)
min(location$MapLongitude ,na.rm=TRUE)
max(location$MapLongitude ,na.rm=TRUE)
#-----------#
#-- Look for nearby --#
# Get long and lat for the address
loci_lat<-mean(location$MapLatitude ,na.rm=TRUE)
loci_lon<-mean(location$MapLongitude ,na.rm=TRUE)
# Set Manual
#-loci_lat<-29.685243003232415
#-loci_lon<- -95.47805340848808
# Get degrees for distance around the location coords
lat_range<-range/111111
lon_range<-(range/111111)#*(cos((loci_lat*3.1415926)/180))
#If your displacements aren't too great (less than a few kilometers) and you're not right at the
#poles, use the quick and dirty estimate that 111,111 meters (111.111 km) in the y direction is 1
#degree (of latitude) and 111,111 * cos(latitude) meters in the x direction is 1 degree (of
#longitude).
#  10 degrees = 10 * pi / 180 radians
box_lon_min<-loci_lon-lon_range
box_lon_max<-loci_lon+lon_range
box_lat_min<-loci_lat-lat_range
box_lat_max<-loci_lat+lat_range
# GET ALL in area
area<-multi_year[(  MapLongitude>=box_lon_min
& MapLongitude<=box_lon_max
& MapLatitude>=box_lat_min
& MapLatitude<=box_lat_max
& year==2023)]
area_sum<-area[,.(OffenseCount=sum(OffenseCount),IncidentCount=.N)
,by=c('NIBRSDescription')]
# Check Locations on a map
plot_map<-area[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))]%>%
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
datax<-st_join( plot_map
,districts
,join = st_within)
geo_plot(datax)
# Tabular Report
area_sum%>%gt()%>%tab_header(
title=glue('HPD Incident Reports 2023 (January-October)'),
subtitle=glue('{sum(area_sum$IncidentCount)} Incidents - {sum(area_sum$OffenseCount)} Offenses'))#%>%
#cols_label( base_chg=md("**ChangeDirection**")
#            ,Freq=md("**Count**")
#            ,Count=md("**Offense Count**"))
#SAVE A CSV
View(area)
library(gt)
range<-500
#4955 BEECHNUT STREET
#4807 Pin Oak Park, Houston, TX 77081
location<-multi_year[(  StreetName=='PIN OAK'
& StreetType=='PRK'
& StreetNo==4807
& year==2023)]
min(location$MapLatitude ,na.rm=TRUE)
max(location$MapLatitude ,na.rm=TRUE)
min(location$MapLongitude ,na.rm=TRUE)
max(location$MapLongitude ,na.rm=TRUE)
#-----------#
#-- Look for nearby --#
# Get long and lat for the address
loci_lat<-mean(location$MapLatitude ,na.rm=TRUE)
loci_lon<-mean(location$MapLongitude ,na.rm=TRUE)
# Set Manual
#-loci_lat<-29.685243003232415
#-loci_lon<- -95.47805340848808
# Get degrees for distance around the location coords
lat_range<-range/111111
lon_range<-(range/111111)#*(cos((loci_lat*3.1415926)/180))
#If your displacements aren't too great (less than a few kilometers) and you're not right at the
#poles, use the quick and dirty estimate that 111,111 meters (111.111 km) in the y direction is 1
#degree (of latitude) and 111,111 * cos(latitude) meters in the x direction is 1 degree (of
#longitude).
#  10 degrees = 10 * pi / 180 radians
box_lon_min<-loci_lon-lon_range
box_lon_max<-loci_lon+lon_range
box_lat_min<-loci_lat-lat_range
box_lat_max<-loci_lat+lat_range
# GET ALL in area
area<-multi_year[(  MapLongitude>=box_lon_min
& MapLongitude<=box_lon_max
& MapLatitude>=box_lat_min
& MapLatitude<=box_lat_max
& year==2023)]
area_sum<-area[,.(OffenseCount=sum(OffenseCount),IncidentCount=.N)
,by=c('NIBRSDescription')]
# Check Locations on a map
plot_map<-area[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))]%>%
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
datax<-st_join( plot_map
,districts
,join = st_within)
geo_plot(datax)
# Tabular Report
area_sum%>%gt()%>%tab_header(
title=glue('HPD Incident Reports 2023 (January-October)'),
subtitle=glue('{sum(area_sum$IncidentCount)} Incidents - {sum(area_sum$OffenseCount)} Offenses'))#%>%
#cols_label( base_chg=md("**ChangeDirection**")
#            ,Freq=md("**Count**")
#            ,Count=md("**Offense Count**"))
#SAVE A CSV
View(districts)
View(location)
library(gt)
range<-500
#4955 BEECHNUT STREET
#4807 Pin Oak Park, Houston, TX 77081
location<-multi_year[(  StreetName=='PIN OAK'
& StreetType=='PRK'
& StreetNo==4807
& year==2023)]
min(location$MapLatitude ,na.rm=TRUE)
max(location$MapLatitude ,na.rm=TRUE)
min(location$MapLongitude ,na.rm=TRUE)
max(location$MapLongitude ,na.rm=TRUE)
#-----------#
#-- Look for nearby --#
# Get long and lat for the address
loci_lat<-mean(location$MapLatitude ,na.rm=TRUE)
loci_lon<-mean(location$MapLongitude ,na.rm=TRUE)
# Set Manual
#-loci_lat<-29.685243003232415
#-loci_lon<- -95.47805340848808
# Get degrees for distance around the location coords
lat_range<-range/111111
lon_range<-(range/111111)#*(cos((loci_lat*3.1415926)/180))
#If your displacements aren't too great (less than a few kilometers) and you're not right at the
#poles, use the quick and dirty estimate that 111,111 meters (111.111 km) in the y direction is 1
#degree (of latitude) and 111,111 * cos(latitude) meters in the x direction is 1 degree (of
#longitude).
#  10 degrees = 10 * pi / 180 radians
box_lon_min<-loci_lon-lon_range
box_lon_max<-loci_lon+lon_range
box_lat_min<-loci_lat-lat_range
box_lat_max<-loci_lat+lat_range
# GET ALL in area
area<-multi_year[(  MapLongitude>=box_lon_min
& MapLongitude<=box_lon_max
& MapLatitude>=box_lat_min
& MapLatitude<=box_lat_max
& year==2023)]
area_sum<-area[,.(OffenseCount=sum(OffenseCount),IncidentCount=.N)
,by=c('NIBRSDescription')]
# Check Locations on a map
plot_map<-area[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))]%>%
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
datax<-st_join( plot_map
,districts
,join = st_within)
geo_plot(datax)
# Tabular Report
area_sum%>%gt()%>%tab_header(
title=glue('HPD Incident Reports 2023 (January-October)'),
subtitle=glue('{sum(area_sum$IncidentCount)} Incidents - {sum(area_sum$OffenseCount)} Offenses'))#%>%
#cols_label( base_chg=md("**ChangeDirection**")
#            ,Freq=md("**Count**")
#            ,Count=md("**Offense Count**"))
#SAVE A CSV
