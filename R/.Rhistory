View(sum_grid)
library(gganimate)
#Maybe all 'major crimes' might be interesting for market basket analysis
city_raw<-read_sf('C:/Users/chris/Documents/Random_Nextdoor/My_Crime_Analysis - 05.04.23/Group/Data/COH_ADMINISTRATIVE_BOUNDARY_-_MIL/COH_ADMINISTRATIVE_BOUNDARY_-_MIL.shp')
city<-city_raw$geometry
#plot(city)
# make a square grid over the countries
grd<-st_make_grid( city
,n = 200
,square=FALSE)
#plot(grd)
# find which grid points intersect `polygons` (countries)
# and create an index to subset from
index<-which(lengths(st_intersects(grd,city))>0)
# subset the grid to make a fishnet
houston_grid<-grd[index]
# visualize the fishnet
plot(houston_grid)
# DATA - Last Complete Year 2022
data_pre<-multi_year[ (NIBRSDescription %chin% violent_crimes)
&(RMSOccurrenceDate>=glue('2022-02-01'))
&(RMSOccurrenceDate<=glue('2022-02-28')),]%>%
.[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))] |>
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
#Look at 123 rows lost to no long/lati
lat_min<-min(data_pre$MapLatitude)
lat_max<-max(data_pre$MapLatitude)
lon_min<-min(data_pre$MapLongitude)
lon_max<-max(data_pre$MapLongitude)
# Join grid cells to incident data
incident_cells<-houston_grid%>%
st_as_sf() |> # cast to sf
mutate(grid_id=row_number()) |> # create unique ID
st_join(data_pre)
# Summarize Incidents by grid-cell
# --Set nulls to zeros
sum_grid<-incident_cells%>%
group_by(grid_id)%>%
summarise(n=sum(OffenseCount))%>%
replace_na(list(n=0))
hist(sum_grid$n)
#-----
print(nrow(sum_grid))
#Drop Neighborless grid-cell
sum_grid<-sum_grid |>
filter(grid_id!=4057)
print(nrow(sum_grid))
sum_grid$nb<-st_contiguity(sum_grid$x)
sum_grid$nn<-lengths(sum_grid$nb)
sum_grid$wt<-st_weights(sum_grid$nb)
sum_grid$n_lag<-st_lag(sum_grid$n,sum_grid$nb,sum_grid$wt)
sum_grid%>%ggplot(aes(fill = n_lag)) +
geom_sf(lwd = 0.1, color = "white")
# Omnibus Clustering test
global_g_test(sum_grid$n,sum_grid$nb,sum_grid$wt)
# Local Cluster detection
sum_grid$Gi<-local_g_perm(sum_grid$n,sum_grid$nb,sum_grid$wt,nsim=199)
spots<-sum_grid%>%unnest(Gi)
gg <- spots |>
select(gi, p_folded_sim) |>
mutate(
classification = case_when(
gi > 0 & p_folded_sim <= 0.01 ~ "Very hot",
gi > 0 & p_folded_sim <= 0.05 ~ "Hot",
gi > 0 & p_folded_sim <= 0.1 ~ "Somewhat hot",
gi < 0 & p_folded_sim <= 0.01 ~ "Very cold",
gi < 0 & p_folded_sim <= 0.05 ~ "Cold",
gi < 0 & p_folded_sim <= 0.1 ~ "Somewhat cold",
TRUE ~ "Insignificant"
),
# we now need to make it look better :)
# if we cast to a factor we can make diverging scales easier
classification = factor(
classification,
levels = c("Very hot", "Hot", "Somewhat hot",
"Insignificant",
"Somewhat cold", "Cold", "Very cold")
)
) |>
ggplot(aes(fill = classification)) +
geom_sf(color = "black", lwd = 0.1) +
scale_fill_brewer(type = "div", palette = 5) +
theme_void() +
labs(
fill = "Hot Spot Classification",
title = "Violent Crime Hot Spots Houston"
)
gg
library(gganimate)
#Maybe all 'major crimes' might be interesting for market basket analysis
city_raw<-read_sf('C:/Users/chris/Documents/Random_Nextdoor/My_Crime_Analysis - 05.04.23/Group/Data/COH_ADMINISTRATIVE_BOUNDARY_-_MIL/COH_ADMINISTRATIVE_BOUNDARY_-_MIL.shp')
city<-city_raw$geometry
#plot(city)
# make a square grid over the countries
grd<-st_make_grid( city
,n = 200
,square=FALSE)
#plot(grd)
# find which grid points intersect `polygons` (countries)
# and create an index to subset from
index<-which(lengths(st_intersects(grd,city))>0)
# subset the grid to make a fishnet
houston_grid<-grd[index]
# visualize the fishnet
plot(houston_grid)
# DATA - Last Complete Year 2022
data_pre<-multi_year[ (NIBRSDescription %chin% violent_crimes)
&(RMSOccurrenceDate>=glue('2022-01-01'))
&(RMSOccurrenceDate<=glue('2023-08-31')),]%>%
.[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))] |>
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
#Look at 123 rows lost to no long/lati
lat_min<-min(data_pre$MapLatitude)
lat_max<-max(data_pre$MapLatitude)
lon_min<-min(data_pre$MapLongitude)
lon_max<-max(data_pre$MapLongitude)
# Join grid cells to incident data
incident_cells<-houston_grid%>%
st_as_sf() |> # cast to sf
mutate(grid_id=row_number()) |> # create unique ID
st_join(data_pre)
# Summarize Incidents by grid-cell
# --Set nulls to zeros
sum_grid<-incident_cells%>%
group_by(grid_id)%>%
summarise(n=sum(OffenseCount))%>%
replace_na(list(n=0))
hist(sum_grid$n)
#-----
print(nrow(sum_grid))
#Drop Neighborless grid-cell
sum_grid<-sum_grid |>
filter(grid_id!=4057)
print(nrow(sum_grid))
sum_grid$nb<-st_contiguity(sum_grid$x)
sum_grid$nn<-lengths(sum_grid$nb)
sum_grid$wt<-st_weights(sum_grid$nb)
sum_grid$n_lag<-st_lag(sum_grid$n,sum_grid$nb,sum_grid$wt)
sum_grid%>%ggplot(aes(fill = n_lag)) +
geom_sf(lwd = 0.1, color = "white")
# Omnibus Clustering test
global_g_test(sum_grid$n,sum_grid$nb,sum_grid$wt)
# Local Cluster detection
sum_grid$Gi<-local_g_perm(sum_grid$n,sum_grid$nb,sum_grid$wt,nsim=199)
spots<-sum_grid%>%unnest(Gi)
gg <- spots |>
select(gi, p_folded_sim) |>
mutate(
classification = case_when(
gi > 0 & p_folded_sim <= 0.01 ~ "Very hot",
gi > 0 & p_folded_sim <= 0.05 ~ "Hot",
gi > 0 & p_folded_sim <= 0.1 ~ "Somewhat hot",
gi < 0 & p_folded_sim <= 0.01 ~ "Very cold",
gi < 0 & p_folded_sim <= 0.05 ~ "Cold",
gi < 0 & p_folded_sim <= 0.1 ~ "Somewhat cold",
TRUE ~ "Insignificant"
),
# we now need to make it look better :)
# if we cast to a factor we can make diverging scales easier
classification = factor(
classification,
levels = c("Very hot", "Hot", "Somewhat hot",
"Insignificant",
"Somewhat cold", "Cold", "Very cold")
)
) |>
ggplot(aes(fill = classification)) +
geom_sf(color = "black", lwd = 0.1) +
scale_fill_brewer(type = "div", palette = 5) +
theme_void() +
labs(
fill = "Hot Spot Classification",
title = "Violent Crime Hot Spots Houston"
)
gg
gg <- spots |>
select(gi, p_folded_sim) |>
mutate(
classification = case_when(
gi > 0 & p_folded_sim <= 0.01 ~ "Very hot",
gi > 0 & p_folded_sim <= 0.05 ~ "Hot",
gi > 0 & p_folded_sim <= 0.1 ~ "Somewhat hot",
gi < 0 & p_folded_sim <= 0.01 ~ "Very cold",
gi < 0 & p_folded_sim <= 0.05 ~ "Cold",
gi < 0 & p_folded_sim <= 0.1 ~ "Somewhat cold",
TRUE ~ "Insignificant"
),
# we now need to make it look better :)
# if we cast to a factor we can make diverging scales easier
classification = factor(
classification,
levels = c("Very hot", "Hot", "Somewhat hot",
"Insignificant",
"Somewhat cold", "Cold", "Very cold")
)
)
View(gg)
# Join grid cells to incident data
incident_cells<-houston_grid%>%
st_as_sf() |> # cast to sf
mutate(grid_id=row_number()) |> # create unique ID
st_join(data_pre)|>
filter(grid_id!=4057)
source("~/GitHub/HoustonCrimeDataAnalysis/R/NIBRS_HPD_ETL.R")
library(gganimate)
#Maybe all 'major crimes' might be interesting for market basket analysis
city_raw<-read_sf('C:/Users/chris/Documents/Random_Nextdoor/My_Crime_Analysis - 05.04.23/Group/Data/COH_ADMINISTRATIVE_BOUNDARY_-_MIL/COH_ADMINISTRATIVE_BOUNDARY_-_MIL.shp')
city<-city_raw$geometry
#plot(city)
# make a square grid over the countries
grd<-st_make_grid( city
,n = 200
,square=FALSE)
#plot(grd)
# find which grid points intersect `polygons` (countries)
# and create an index to subset from
index<-which(lengths(st_intersects(grd,city))>0)
# subset the grid to make a fishnet
houston_grid<-grd[index]
# visualize the fishnet
plot(houston_grid)
# DATA - Last Complete Year 2022
data_pre<-multi_year[ (NIBRSDescription %chin% violent_crimes)
&(RMSOccurrenceDate>=glue('2022-01-01'))
&(RMSOccurrenceDate<=glue('2023-08-31')),]%>%
.[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))] |>
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
#Look at 123 rows lost to no long/lati
lat_min<-min(data_pre$MapLatitude)
lat_max<-max(data_pre$MapLatitude)
lon_min<-min(data_pre$MapLongitude)
lon_max<-max(data_pre$MapLongitude)
# Join grid cells to incident data
incident_cells<-houston_grid%>%
st_as_sf() |> # cast to sf
mutate(grid_id=row_number()) |> # create unique ID
st_join(data_pre)|>
filter(grid_id!=4057) ## Drop Neighborless grid-cell - Empircally derived
# Summarize Incidents by grid-cell
# --Set nulls to zeros
sum_grid<-incident_cells%>%
group_by(grid_id)%>%
summarise(n=sum(OffenseCount))%>%
replace_na(list(n=0))
hist(sum_grid$n)
#-----
sum_grid$nb<-st_contiguity(sum_grid$x)
sum_grid$nn<-lengths(sum_grid$nb)
sum_grid$wt<-st_weights(sum_grid$nb)
sum_grid$n_lag<-st_lag(sum_grid$n,sum_grid$nb,sum_grid$wt)
sum_grid%>%ggplot(aes(fill = n_lag)) +
geom_sf(lwd = 0.1, color = "white")
# Omnibus Clustering test
global_g_test(sum_grid$n,sum_grid$nb,sum_grid$wt)
# Local Cluster detection
sum_grid$Gi<-local_g_perm(sum_grid$n,sum_grid$nb,sum_grid$wt,nsim=199)
spots<-sum_grid%>%unnest(Gi)
gg <- spots |>
select(gi, p_folded_sim) |>
mutate(
classification = case_when(
gi > 0 & p_folded_sim <= 0.01 ~ "Very hot",
gi > 0 & p_folded_sim <= 0.05 ~ "Hot",
gi > 0 & p_folded_sim <= 0.1 ~ "Somewhat hot",
gi < 0 & p_folded_sim <= 0.01 ~ "Very cold",
gi < 0 & p_folded_sim <= 0.05 ~ "Cold",
gi < 0 & p_folded_sim <= 0.1 ~ "Somewhat cold",
TRUE ~ "Insignificant"
),
# we now need to make it look better :)
# if we cast to a factor we can make diverging scales easier
classification = factor(
classification,
levels = c("Very hot", "Hot", "Somewhat hot",
"Insignificant",
"Somewhat cold", "Cold", "Very cold")
)
) |>
ggplot(aes(fill = classification)) +
geom_sf(color = "black", lwd = 0.1) +
scale_fill_brewer(type = "div", palette = 5) +
theme_void() +
labs(
fill = "Hot Spot Classification",
title = "Violent Crime Hot Spots Houston"
)
gg
## Emerging Hot-Spot Analysis
# Create spacetime object called `bos`
time_series<-
geo_data<-
cube<-spacetime( df
,geo
,.loc_col = ".region_id"
,.time_col = "time_period")
View(incident_cells)
## Emerging Hot-Spot Analysis
# Create spacetime object called `bos`
# Summarize Incidents by grid-cell
# --Set nulls to zeros
time_series<-incident_cells%>%
group_by(grid_id,year_mon)%>%
summarise(n=sum(OffenseCount))%>%
replace_na(list(n=0))
df_fp <- system.file("extdata", "bos-ecometric.csv", package = "sfdep")
geo_fp <- system.file("extdata", "bos-ecometric.geojson", package = "sfdep")
# read in data
df <- read.csv(df_fp, colClasses = c("character", "character", "integer", "double", "Date"))
geo <- sf::st_read(geo_fp)
View(geo)
View(sum_grid)
cube<-spacetime( time_series
,geo_data
,.loc_col="grid_id"
,.time_col="year_mon")
geo_data<-sum_grid[,.(grid_id,x)]
geo_data<-sum_grid[(grid_id,x)]
geo_data<-sum_grid[c(grid_id,x)]
geo_data<-sum_grid[c('grid_id','x')]
cube<-spacetime( time_series
,geo_data
,.loc_col="grid_id"
,.time_col="year_mon")
# conduct EHSA
ehsa <- emerging_hotspot_analysis(
x = cube,
.var = "n",
k = 1,
nsim = 499
)
cube<-spacetime( time_series
,geo_data
,.loc_col="grid_id"
,.time_col="year_mon")
View(cube)
# Summarize Incidents by grid-cell
# --Set nulls to zeros
time_series<-incident_cells%>%
group_by(grid_id,year_mon)%>%
summarise(n=sum(OffenseCount))%>%
replace_na(list(n=0))
geo_data<-sum_grid[c('grid_id','x')]
cube<-spacetime( time_series
,geo_data
,.loc_col="grid_id"
,.time_col="year_mon")
# conduct EHSA
ehsa <- emerging_hotspot_analysis(
x = cube,
.var = "n",
k = 1,
nsim = 499
)
df_fp <- system.file("extdata", "bos-ecometric.csv", package = "sfdep")
geo_fp <- system.file("extdata", "bos-ecometric.geojson", package = "sfdep")
# read in data
df <- read.csv(df_fp, colClasses = c("character", "character", "integer", "double", "Date"))
geo <- sf::st_read(geo_fp)
# Create spacetime object called `bos`
bos <- spacetime(df, geo,
.loc_col = ".region_id",
.time_col = "time_period")
View(df)
View(time_series)
View(incident_cells)
View(time_series)
View(cube)
View(df)
View(df)
View(time_series)
library(gganimate)
#Maybe all 'major crimes' might be interesting for market basket analysis
city_raw<-read_sf('C:/Users/chris/Documents/Random_Nextdoor/My_Crime_Analysis - 05.04.23/Group/Data/COH_ADMINISTRATIVE_BOUNDARY_-_MIL/COH_ADMINISTRATIVE_BOUNDARY_-_MIL.shp')
city<-city_raw$geometry
#plot(city)
# make a square grid over the countries
grd<-st_make_grid( city
,n = 200
,square=FALSE)
#plot(grd)
# find which grid points intersect `polygons` (countries)
# and create an index to subset from
index<-which(lengths(st_intersects(grd,city))>0)
# subset the grid to make a fishnet
houston_grid<-grd[index]
# visualize the fishnet
plot(houston_grid)
# DATA - Last Complete Year 2022
data_pre<-multi_year[ (NIBRSDescription %chin% violent_crimes)
&(RMSOccurrenceDate>=glue('2022-01-01'))
&(RMSOccurrenceDate<=glue('2023-08-31')),]%>%
.[ (!is.na(MapLongitude))
|(!is.na(MapLatitude))] |>
st_as_sf( coords=c("MapLongitude","MapLatitude")
,crs=4326
,remove=FALSE)
#Look at 123 rows lost to no long/lati
lat_min<-min(data_pre$MapLatitude)
lat_max<-max(data_pre$MapLatitude)
lon_min<-min(data_pre$MapLongitude)
lon_max<-max(data_pre$MapLongitude)
# Join grid cells to incident data
incident_cells<-houston_grid%>%
st_as_sf() |> # cast to sf
mutate(grid_id=row_number()) |> # create unique ID
st_join(data_pre)|>
filter(grid_id!=4057) ## Drop Neighborless grid-cell - Empircally derived
# Summarize Incidents by grid-cell
# --Set nulls to zeros
sum_grid<-incident_cells%>%
group_by(grid_id)%>%
summarise(n=sum(OffenseCount))%>%
replace_na(list(n=0))
hist(sum_grid$n)
#-----
sum_grid$nb<-st_contiguity(sum_grid$x)
sum_grid$nn<-lengths(sum_grid$nb)
sum_grid$wt<-st_weights(sum_grid$nb)
sum_grid$n_lag<-st_lag(sum_grid$n,sum_grid$nb,sum_grid$wt)
sum_grid%>%ggplot(aes(fill = n_lag)) +
geom_sf(lwd = 0.1, color = "white")
# Omnibus Clustering test
global_g_test(sum_grid$n,sum_grid$nb,sum_grid$wt)
# Local Cluster detection
sum_grid$Gi<-local_g_perm(sum_grid$n,sum_grid$nb,sum_grid$wt,nsim=199)
spots<-sum_grid%>%unnest(Gi)
gg <- spots |>
select(gi, p_folded_sim) |>
mutate(
classification = case_when(
gi > 0 & p_folded_sim <= 0.01 ~ "Very hot",
gi > 0 & p_folded_sim <= 0.05 ~ "Hot",
gi > 0 & p_folded_sim <= 0.1 ~ "Somewhat hot",
gi < 0 & p_folded_sim <= 0.01 ~ "Very cold",
gi < 0 & p_folded_sim <= 0.05 ~ "Cold",
gi < 0 & p_folded_sim <= 0.1 ~ "Somewhat cold",
TRUE ~ "Insignificant"
),
# we now need to make it look better :)
# if we cast to a factor we can make diverging scales easier
classification = factor(
classification,
levels = c("Very hot", "Hot", "Somewhat hot",
"Insignificant",
"Somewhat cold", "Cold", "Very cold")
)
) |>
ggplot(aes(fill = classification)) +
geom_sf(color = "black", lwd = 0.1) +
scale_fill_brewer(type = "div", palette = 5) +
theme_void() +
labs(
fill = "Hot Spot Classification",
title = "Violent Crime Hot Spots Houston"
)
gg
View(time_series)
View(df)
## ACCESS DATA FROM:
library(pdftools)
library(tidyverse)
library(readr)
library(glue)
library(stringr)
library(data.table)
library(networkD3)
library(ggplot2)
library(lubridate)
out_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/downloaded/'
text_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/text_conv/'
edit_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/hand_cleaned/'
# Read list of pdf_files to download
BPD_DATA <- read_csv("Bellaire_Crime_Data/BPD_DATA_v004.txt",col_names=FALSE)
## ACCESS DATA FROM:
library(pdftools)
library(tidyverse)
library(readr)
library(glue)
library(stringr)
library(data.table)
library(networkD3)
library(ggplot2)
library(lubridate)
out_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/downloaded/'
text_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/text_conv/'
edit_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/hand_cleaned/'
# Read list of pdf_files to download
BPD_DATA <- read_csv("Bellaire_Crime_Data/BPD_DATA_v004.txt",col_names=FALSE)
## ACCESS DATA FROM:
library(pdftools)
library(tidyverse)
library(readr)
library(glue)
library(stringr)
library(data.table)
library(networkD3)
library(ggplot2)
library(lubridate)
out_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/downloaded/'
text_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/text_conv/'
edit_lib<-'C:/Users/chris/Documents/Bellaire_Crime_Data/hand_cleaned/'
# Read list of pdf_files to download
BPD_DATA <- read_csv("Bellaire_Crime_Data/BPD_DATA.txt",col_names=FALSE)
